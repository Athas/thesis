\chapter{Introduction}

For practically the entire lifetime of the electronic computer,
programmers have been used to an exponential growth in commonly
available computing power.  Until around 2006, this directly
manifested itself as improvements to sequential performance, although
physical limits made it uneconomical (or impossible) for this trend to
continue.  These days, hardware designers are making their machines
increasingly \textit{parallel}: rather than speeding up the individual
processors, as happened previously, \textit{more} processors, or more
\textit{specialised} processors, are added.  Thus, while computing
power is still growing, it has become increasingly necessary to write
programs that are parallel in order to take full advantage of modern
advancements in hardware.

One interesting development is the commoditisation of massively
parallel vector processors in the form of graphics cards.  While
hardware acceleration of graphics became commonplace in the 90s, it
was not until the rise of CUDA and OpenCL in 2006 that
\textit{General-Purpose computing on Graphics Processing Units}
(GPGPU) began to move into the mainstream.  The kind of parallelism
supported by GPGPU is \textit{data parallelism}, wherein each
processor performs the same task on different pieces of the data.
This is also called Single Instruction Multiple Data (SIMD).  Today,
there are three main ways to take advantage of this data parallel
processing power:

\begin{description}
\item[Low-level interfaces:] CUDA and OpenCL implementations are
  supplied by the GPU vendors.\footnote{Strictly. OpenCL has a broader
    focus, and seeks to provide an interface to heterogenous
    computation in general, but for the purpose of this thesis, we
    will consider OpenCL and CUDA to be GPU-oriented.}  These are very
  low-level, and provide a C-like programming interface.  Furthermore,
  GPU hardware has very complicated performance characteristics, and
  it can be hard to achieve optimal, or even good performance.
  Nevertheless, the full supported power of the devices is available
  at this level, and optimal performance is theoretically achievable,
  although in practice, specialist knowledge is required to achieve
  good results at this level.

  At the most basic level, all other approaches eventually boil down
  to talking to a low-level interface.

\item[Libraries:] Some programming libraries aimed at high-peformance
  computing have been rewritten to take advantage of GPU acceleration.
  For example, Nvidia provides CUBLAS~\cite{nvidia2008cublas}, an
  implementation of the well-known BLAS array operations API.  These
  libraries are typically written by experts, and come close to peak
  potential performance on the target hardware.  It is generally easy
  to use these libraries from any language offering a good foreign
  function interface, and it is thus an efficient way to reach a large
  number of potential users.  Usage of these libraries requires little
  in the way of GPU knowledge, or indeed knowledge about parallel
  programming at all.

  On the downside, although each discrete function may be
  well-optimised in isolation, the library approach does not permit
  optimisation of composed operations.  For example, if a library
  exports a function \texttt{mult} to multiply two matrices, and we
  use it in two invocations to multiply three matrices, as in
  \texttt{mult(x,mult(y,z))}, the library will likely not be as fast
  as if we used a specialised ternary multiplication function.
  Although particularly clever libraries may use a variant of lazy
  evaluation to delay computation and optimise some composed
  operations~\cite{kristensenbohrium}, the optimisation potential is
  still limited as long as the program cannot be inspected directly.

  The library approach is very popular in practice, with many
  high-performance computing libraries now possessing GPGPU backends.

\item[Data-parallel programming languages:] The final way to perform
  GPGPU is to integrate GPU support directly into a programming
  language, with full compiler support.  This permits code generation
  based on a global view of the entire computation, at least in
  theory, and optimise with full knowledge of the program.  There
  appears to be two main paths within the programming language
  approach:

\begin{description}
\item[Embedded languages:] Somewhat similar to the library approach,
  this integrates GPGPU support in an existing language as an
  \textit{Embedded Domain Specific Language}
  (EDSL)~\cite{czarnecki2004dsl}.  The distinction between an EDSL and
  a library is often fuzzy, with the distinction typically being about
  the level of composability offered, and whether the EDSL follows the
  same evaluation rules as the host language.  Further blurring the
  issue, some EDSLs use syntactical extensions -- for example through
  macros~\cite{kohlbecker1986hygienic} or
  quasiquotation~\cite{mainland2007s} -- while others take advantage
  of the host languages existing syntactical facilities.

  The limitations for EDSLs are similar to the ones for libraries.
  For example, the embedded language must be expressible in the type
  system of the host language.  It also varies how much support the
  host language provides for hooking into the compiler, in order to
  perform optimisation.  On the other hand, much of the infrastructure
  of the host language will be inherited by the EDSL, leading to a
  much simpler implementation, compared to writing a full compiler.
  Furthermore, due to the integration with the host language, EDSL
  usage can be very seamless.  On the other hand, it is extremely hard
  to access an EDSL from outside of the host language.

  DPH, Repa and
  Accelerate~\cite{Chak06DPH,keller2010regular,ArrayAccelerate} are
  examples of data-parallel EDSLs for Haskell, with Accelerate suppors
  OpenCL and CUDA backends.

\item[Independent languages:] The final approach is to write an entire
  compiler targeting GPGPU execution.  This provides total control, at
  the cost of greatly increased implementation complexity.
  Furthermore, it can be difficult to integrate components written in
  a these new languages into existing code-bases written in mainstream
  languages.  Nevertheless, the language can be designed from the
  bottom up for efficient parallel execution, without compromises due
  to host language integration.  The NESL~\cite{BlellochCACM96NESL}
  language is an early ('96) example of a programming language
  designed entirely for data-parallel execution.  Although designed
  before the proliferation of GPUs, a GPU backend has recently been
  developed~\cite{bergstrom2012nested}.  Another example of such a
  language is Single-Assignment C (SAC)~\cite{grelck2006sac}.

  In a way, we could also consider the OpenCL and CUDA kernel
  languages themselves to be in this category, but we only consider
  high-level languages to be proper members of this group.
\end{description}
\end{description}

The library approach is effective if a library exists for the specific
problem the programmer is attempting to solve, but will often be
neither sufficiently fast nor expressive for new domains.  EDSLs
suffer from a similar problem -- in particular, nested parallelism and
similar complex control flow is generally poorly supported.  The NESL
and SAC languages are more expressive, but their implementation does
not perform many advanced optimisations.  Clearly, there is still
great uncertainty about the best way to program these massively
parallel machines.

To investigate possible solutions, we have examined several real-world
financial kernels originally implemented in languages such as OCaml,
C++, and C, and measuring in the range of hundreds of lines of compact
code, with two main objectives in mind:

\begin{enumerate}
\item What is the simplest language that permits a relatively
  straightforward translation of the financial programs, while still
  expressing algorithm invariants that enable the generation of
  efficient parallel code?

\item What compiler optimizations would result in efficiency
  comparable to code hand-tuned for the specific hardware?
\end{enumerate}

To answer the first question, we will present \LO{}, an independent
language designed for parallel execution.  We have chosen to implement
\LO{} as a non-embedded language in order to have more design freedom,
as we do not need to, e.g., fit \LO{} into the type system of an
existing language.  Our language supports \textit{nested} parallelism
on \textit{regular} arrays, i.e., arrays where all rows of the array
have the same size.  This restriction is due to regular arrays being
more amenable to compiler optimizations, in particular they allow
transposition and simplified size analysis.

Our language supports nested parallelism because many programs
exhibits several layers of parallelism that cannot be exploited by
flat parallelism in the style of REPA~\cite{keller2010regular}.  For
example, the examined financial kernels contain several innermost
\texttt{scan} or \texttt{reduce} operations, and at least one
semantically sequential loop per benchmark.

Our language is also \textit{functional}, because we would rather
invest compiler effort in exploiting high-level program invariants
rather than in proving them.  The common example here is parallelism:
\texttt{map-reduce} constructs are inherently parallel, while
Fortran-style \texttt{do} loops require sophisticated analysis to
decide parallelism.  Furthermore, such
analyses~\cite{Blume94RangeTest,SUIF,CosPLDI,SummaryMonot} have not
yet been integrated in the repertoire of commercial compilers, likely
due to ``heroic effort'' concerns, albeit their effectiveness was
demonstrated on comprehensive suites, and some of them were developed
more than a decade ago.

The answer to the second question seems to be that a common ground
needs to be found between functional and imperative optimizations and,
to a lesser extent, between functional and imperative language
constructs.  Much in the same way in which data parallelism seems to
be generated by a combination of \texttt{map}, \texttt{reduce}, and
\texttt{scan} operations, the optimization opportunities seem solvable
via a combination of \textit{transposition}, loop \textit{fusion},
loop \textit{interchange} and loop
\textit{distribution}~\cite{OptCompModernArch}.

It follows that classic index-based loops are necessary in the
intermediate representation, regardless of whether they are provided
as a language construct or are derived from tail-recursive functions
via a code transformation.

Loop fusion is one of the most important code transformations, as it
has the potential for substantially optimising both memory hierarchy
overhead and, sometimes asymptotically, space requirements.  In
imperative languages, fusing producer-consumer loops requires
dependency analysis on arrays applied at loop-nest level.  Such
analysis, however, has often been labeled as ``heroic effort'' and, if
at all, is supported only in its simplest and most conservative form
in industrial compilers.  In functional languages however, fusion is
naturally and relatively easily derived from the producer-consumer
relation between program constructs that expose a rich, higher-order
algebra of program invariants, such as the \texttt{map-reduce} list
homomorphisms.

Finally, an indirect consequence of having to deal with sequential
dependent loops is that \LO{} provides support for \textit{in-place
  updates} of array elements.  The observable semantics still respect
referential transparency, i.e., a deep copy of the original array but
with the corresponding element replaced, \textit{intersected} with the
imperative one, i.e., referential transparency cannot be guaranteed, a
compile-time error is signaled.  This approach enables the intuitive
cost model that the user likely assumes, while preserving the
functional semantics.

Throughout this thesis, we will often refer to a vaguely defined
``programmer'', as well as ascribe various motives and expectations to
this nebulous being.  While \LO{} is intended as an intermediate
language, and in the end is intended as a target language by compilers
for higher-level languages, it has a well-defined human-readable (and
writable) syntax, and can be programmed directly.  Indeed, all extant
\LO{} programs have been written by hand.  Thus, when ``the
programmer'' is referenced, we can refer to either an actual human, or
a compiler generating \LO{} code.  For our purposes, these will have
identical motives, although a human programmer may complain somewhat
more vocally about the lack of syntactical niceties in the language.

\section{Contributions}

We present a purely functional data-parallel programming language,
\LO{}, with support for nested parallelism.  The language supports a
method for safely performing in-place updates of array data through a
type system concept called \textit{uniqueness types}.  Through the
translation of real-world financial programs to \LO{}, we demonstrate
the practical usefulness of this language feature.

We describe the design and implementation of several optimisations,
notably hoisting bounds checks out of inner loops, and loop fusion
based on a structural transformation.  The fusion transformation is
capable of fusing loops whose output is used in multiple places, when
possible without duplicating computation.  Optimising bounds checks is
an example of a general principle of removing checks statically when
possible, and dynamically when necessary.

The benefits of our optimisations are demonstrated on three real-world
financial benchmarks.  It is shown that the compiler is able to hoist
bounds checks and other assertions outside of loops.

The effectiveness of fusion is demonstrated via compiler
instrumentation and quantitative and qualitative measurements on the
three benchmarks, in the form of inspecting the changes in program
dataflow.  This shows that always refusing to duplicate computation is
too conservative on parallel hardware, and discuss potential
directions for further improvement.

The implementation of the \LO{} compiler consists of roughly ten
thousand lines of Haskell (ignoring comments and blank lines), and it
is hosted and publicly browsable at
\url{https://github.com/HIPERFIT/L0Language}.

Parts of this thesis, in particular the core of the fusion algorithm
in \cref{chap:fusion}, has been previously published as

\begin{quote}
\fullcite{Henriksen:2013:TGA:2502323.2502328}
\end{quote}

\section{Report outline}

The remainder of the report is structured as follows.
\Cref{chap:l0language,chap:uniqueness-types} will introduce the
programmer-visible part of \LO{} and serves as a language reference.
\Cref{chap:internal} presents a slight modification of the external
language, that makes it more amenable to transformation and
optimisation.  \Cref{chap:first-order-optimisations} discusses simple
classical optimisations in the context of \LO{}, while
\cref{chap:rebinder} discusses slightly more advanced classical
optimisations.  \Cref{chap:fusion} covers \textit{loop fusion}, an
important structural optimisation, while
\cref{chap:fusion-enabling-soac-transformations,chap:hindrance-removal}
cover transformations that enable other optimisations (although
particularly fusion).

\section{Notation}

In various places, I will use an \(\overline{\text{overline}}\) to
indicate a comma-separated sequence of terms.  For example, when
describing a function call, rather than writing:
\[
f(e_{1},\ldots,e_{n})
\]
I may instead write:
\[
f(\overline{es})
\]
I may also use this in conjunction with expliclt arguments, as in:
\[
f(e_{start},\overline{es},e_{end})
\]
Which is a shortcut for
\[
f(e_{start},e_{1},\ldots,e_{n},e_{end})
\]

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
