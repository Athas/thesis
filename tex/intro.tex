\chapter{Introduction}

For practically the entire lifetime of the electronic computer,
programmers have been used to an exponential growth in commonly
available computing power.  Until around 2006, this directly
manifested itself as improvements to sequential performance, although
physical limits made it uneconomical (or impossible) for this trend to
continue.  These days, hardware designers are making their machines
increasingly \textit{parallel}: rather than speeding up the individual
processors, as happened previously, \textit{more} processors, or more
\textit{specialised} processors, are added.  Thus, while computing
power is still growing, it has become increasingly necessary to write
programs that are parallel in order to take full advantage of modern
advancements in hardware.

One interesting development is the commoditisation of massively
parallel vector processors in the form of graphics cards.  While
hardware acceleration of graphics became commonplace in the 90s, it
was not until the rise of CUDA and OpenCL in 2006 that
\textit{General-Purpose computing on Graphics Processing Units}
(GPGPU) began to move into the mainstream.  Today, there are three
main ways to take advantage of this parallel processing power:

\begin{description}
\item[Low-level interfaces:] CUDA and OpenCL implementations are
  supplied by the GPU vendors.\footnote{Strictly. OpenCL has a broader
    focus, and seeks to provide an interface to heterogenous
    computation in general, but for the purpose of this thesis, we
    will consider OpenCL and CUDA to be GPU-oriented.}  These are very
  low-level, and provide a C-like programming interface.  Furthermore,
  GPU hardware has very complicated performance characteristics, and
  it can be hard to achieve optimal, or even good performance.
  Nevertheless, the full supported power of the devices is available
  at this level, and optimal performance is theoretically achievable,
  although in practice, specialist knowledge is required to achieve
  good results at this level.

  At the most basic level, all other approaches eventually boil down
  to talking to a low-level interface.

\item[Libraries:] Some programming libraries aimed at high-peformance
  computing have been rewritten to take advantage of GPU acceleration.
  For example, Nvidia provides CUBLAS~\cite{nvidia2008cublas}, an
  implementation of the well-known BLAS array operations API.  These
  libraries are typically written by experts, and come close to peak
  potential performance on the target hardware.  It is generally easy
  to use these libraries from any language offering a good foreign
  function interface, and it is thus an efficient way to reach a large
  number of potential users.  Usage of these libraries requires little
  in the way of GPU knowledge, or indeed knowledge about parallel
  programming at all.

  On the downside, although each discrete function may be
  well-optimised in isolation, the library approach does not permit
  optimisation of composed operations.  For example, if a library
  exports a function \texttt{mult} to multiply two matrices, and we
  use it in two invocations to multiply three matrices, as in
  \texttt{mult(x,mult(y,z))}, the library will likely not be as fast
  as if we used a specialised ternary multiplication function.
  Although particularly clever libraries may use a variant of lazy
  evaluation to delay computation and optimise some composed
  operations~\cite{kristensenbohrium}, the optimisation potential is
  still limited as long as the program cannot be inspected directly.

  The library approach is very popular in practice, with many
  high-performance computing libraries now possessing GPGPU backends.

\item[Data-parallel programming languages:] The final way to perform
  GPGPU is to integrate GPU support directly into a programming
  language, with full compiler support.  This permits code generation
  based on a global view of the entire computation, at least in
  theory, and optimise with full knowledge of the program.  There
  appears to be two main paths within the programming language
  approach:

\begin{description}
\item[Embedded languages:] Somewhat similar to the library approach,
  this integrates GPGPU support in an existing language as an
  \textit{Embedded Domain Specific Language}
  (EDSL)~\cite{czarnecki2004dsl}.  The distinction between an EDSL and
  a library is often fuzzy, with the distinction typically being about
  the level of composability offered, and whether the EDSL follows the
  same evaluation rules as the host language.  Further blurring the
  issue, some EDSLs use syntactical extensions -- for example through
  macros~\cite{kohlbecker1986hygienic} or
  quasiquotation~\cite{mainland2007s} -- while others take advantage
  of the host languages existing syntactical facilities.

  The limitations for EDSLs are similar to the ones for libraries.
  For example, the embedded language must be expressible in the type
  system of the host language.  It also varies how much support the
  host language provides for hooking into the compiler, in order to
  perform optimisation.  On the other hand, much of the infrastructure
  of the host language will be inherited by the EDSL, leading to a
  much simpler implementation, compared to writing a full compiler.
  Furthermore, due to the integration with the host language, EDSL
  usage can be very seamless.  On the other hand, it is extremely hard
  to access an EDSL from outside of the host language.

  DPH, Repa and
  Accelerate~\cite{Chak06DPH,keller2010regular,ArrayAccelerate} are
  examples of data-parallel EDSLs for Haskell, with Accelerate suppors
  OpenCL and CUDA backends.

\item[Independent languages:] The final approach is to write an entire
  compiler targeting GPGPU execution.  This provides total control, at
  the cost of greatly increased implementation complexity.
  Furthermore, it can be difficult to integrate components written in
  a these new languages into existing code-bases written in mainstream
  languages.  Nevertheless, the language can be designed from the
  bottom up for efficient parallel execution, without compromises due
  to host language integration.  The NESL~\cite{BlellochCACM96NESL}
  language is an early ('96) example of a programming language
  designed entirely for data-parallel execution.  Although designed
  before the proliferation of GPUs, a GPU backend has recently been
  developed~\cite{bergstrom2012nested}.  Another example of such a
  language is Single-Assignment C (SAC)~\cite{grelck2006sac}.

  In a way, we could also consider the OpenCL and CUDA kernel
  languages themselves to be in this category, but we only consider
  high-level languages to be proper members of this group.
\end{description}
\end{description}

The library approach is effective if a library exists for the specific
problem the programmer is attempting to solve, but will often be
neither sufficiently fast nor expressive for new domains.  EDSL suffer
from a similar problem -- in particular, nested parallelism is
generally very poorly supported.  The NESL and SAC languages are more
expressive, but their implementation does not perform many advanced
optimisations.

In this thesis, we will present \LO{}, an independent language
designed for parallel execution.  We have chosen to implement \LO{} as
a non-embedded language in order to have more design freedom, as we do
not need to, e.g., fit \LO{} into the type system of an existing
language.

Loop fusion is one of the most important code transformations, as it
has the potential for substantially optimising both memory hierarchy
overhead and, sometimes asymptotically, space requirement.  In
functional languages, fusion is naturally and relatively easily
derived from the producer-consumer relation between program constructs
that expose a rich, higher-order algebra of program invariants, such
as the \texttt{map-reduce} list homomorphisms.

In imperative languages, fusing producer-consumer loops requires
dependency analysis on arrays applied at loop-nest level.  Such
analysis, however, has often been labeled as ``heroic effort'' and, if
at all, is supported only in its simplest and most conservative form
in industrial compilers.

... \fixme{Insert more - convince reader that my work addresses an
  important problem, and that I will give a well thought-out solution.
  Why is \LO{} functional, and why does it support sequential loops
  anyway?}

Throughout this thesis, we will often refer to a vaguely defined
``programmer'', as well as ascribe various motives and expectations to
this nebulous being.  While \LO{} is intended as an intermediate
language, and in the end is intended as a target language by compilers
for higher-level languages, it has a well-defined human-readable (and
writable) syntax, and can be programmed directly.  Indeed, all extant
\LO{} programs have been written by hand.  Thus, when ``the
programmer'' is referenced, we can refer to either an actual human, or
a compiler generating \LO{} code.  For our purposes, these will have
identical motives, although a human programmer may complain somewhat
more vocally about the lack of syntactical niceties in the language.

\section{Contributions}

We present a purely functional data-parallel programming language,
\LO{}, with support for nested parallelism.  The language supports a
method \fixme{Why do I support nested parallelism?  Why only regular
  arrays?  Why are in-place updates supported (cross-iteration
  dependent loops)} for safely performing in-place updates of array
data through a type system concept called \textit{uniqueness types}.
Through the translation of real-world financial programs to \LO{}, we
demonstrate the practical usefulness of this language feature.

We describe the design and implementation of several optimisations,
notably hoisting bounds checks out of inner loops, and loop fusion
based on a structural transformation.  The fusion transformation is
capable of fusing loops whose output is used in multiple places, when
possible without duplicating computation.\fixme{Why is it important to
  hoist bounds checks?  Because we target aggressive (static+dynamic)
  analysis; this is an instance of optimising arbitrary predicates.}

The benefits of our optimisations are demonstrated on three real-world
financial benchmarks.  It is shown that the compiler is able to hoist
bounds checks and other assertions outside of loops.

The effectiveness of fusion is demonstrated via compiler
instrumentation and quantitative and qualitative measurements on the
three benchmarks, in the form of inspecting the changes in program
dataflow.  This shows that always refusing to duplicate computation is
too conservative on parallel hardware, and discuss potential
directions for further improvement.

The implementation of the \LO{} compiler consists of roughly ten
thousand lines of Haskell (ignoring comments and blank lines), and it
is hosted and publicly browsable at
\url{https://github.com/HIPERFIT/L0Language}.

Parts of this thesis, in particular the core of the fusion algorithm
in \cref{chap:fusion}, has been previously published as

\begin{quote}
\fullcite{Henriksen:2013:TGA:2502323.2502328}
\end{quote}

\section{Report outline}

The remainder of the report is structured as follows.
\Cref{chap:l0language,chap:uniqueness-types} will introduce the
programmer-visible part of \LO{} and serves as a language reference.
\Cref{chap:internal} presents a slight modification of the external
language, that makes it more amenable to transformation and
optimisation.  \Cref{chap:first-order-optimisations} discusses simple
classical optimisations in the context of \LO{}, while
\cref{chap:rebinder} discusses slightly more advanced clasical
optimisations.  \Cref{chap:fusion} covers \textit{loop fusion}, an
important structural optimisation, while
\cref{chap:fusion-enabling-soac-transformations,chap:hindrance-removal}
cover transformations that enable other optimisations (although
particularly fusion).

\section{Notation}

In various places, I will use an \(\overline{\text{overline}}\) to
indicate a comma-separated sequence of terms.  For example, when
describing a function call, rather than writing:
\[
f(e_{1},\ldots,e_{n})
\]
I may instead write:
\[
f(\overline{es})
\]
I may also use this in conjunction with expliclt arguments, as in:
\[
f(e_{start},\overline{es},e_{end})
\]

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
