\chapter{Conclusions}
\label{chap:conclusions}

This chapter summarises the result of our work.
\Cref{sec:related-work} compares our fusion algorithm with fusion in
other data-parallel programming languages, as well as looking at other
approaches to uniqueness typing.  \Cref{sec:future-work} outlines a
number of possible future improvements to \LO{} and the compiler.
\Cref{sec:conclusion} provides a final summary of the results of this
thesis.

\section{Related Work}
\label{sec:related-work}

Our approach to performining fusion, via rewrite rules, is not unique
by itself, as this is the approach used in e.g. Data-Parallel
Haskell~\cite{Chak06DPH} (DPH).  What sets us apart is the fact that
our rewriting rules are defined on the dataflow graph, and not the
program itself.  While DPH obtains good results, its rewrite rules are
quite limited -- they are an inherently local view of the program, and
would be unable to cope with limitations in the presence of in-place
array updates, and whether the result of an array operation is used
multiple times.  The Glasgow Haskell Compiler itself also bases its
list fusion on rewrite rules and cross-module
inlining~\cite{jones2001playing,gill1993short}.

The Repa~\cite{keller2010regular} approach to fusion is based on a
delayed representation of arrays, which models an array as a function
from index to value.  With this representation, fusion happens
automatically through function composition, although this can cause
duplication of work in many cases.  To counteract this, Repa lets the
user \textit{force} an array, by which it is converted from the
delayed representation to a traditional sequence of values.  The pull
arrays of Obsidian~\cite{claessen2012expressive} use a similar
mechanism.  This approach puts the onus on the programmer to specify
points where the manifestation of arrays is beneficial, even though
this may be a low-level consideration that depends on details of the
target hardware.  We therefore consider this a job better suited for
the compiler.

Accelerate~\cite{mcdonell2013optimising} uses an elaboration of the
delayed arrays representation from Repa, and in particular manages to
avoid duplicating work.  All array operations have a uniform
representation as constructors for delayed arrays, on which fusion is
performed by tree contraction.  Accelerate supports multiple arrays as
input to the same array operation (using a \texttt{zipWith}
construct).  Although arrays are usually used at least twice (once for
getting the size, once for the data), it does not seem that they can
handle the difficult case where the output of an array operation is
used as input to two other array operations.

NESL has been extended with a GPU backend~\cite{bergstrom2012nested},
for which the authors note that fusion is critical to the performance
of the flattened program.  The NESL approach is to use a form of
copy-propagation on the intermediary code, and lift the resulting
functions to work on entire arrays.  This approach only works for what
we would term \texttt{map}-\texttt{map} fusion, however.

Our uniqueness attributes have some similarities to the ``owning
pointers'' found in the impure language Rust~\cite{rust}, albeit there
are deep differences.  In Rust, owning pointers are used to manage
memory -- when an owning pointer goes out of scope, the memory it
points to is deallocated -- while we use uniqueness attributes to
handle side effects.  In addition, we allow function calls to consume
arrays passed as unique-type parameters, whereas in Rust this causes a
deep copy of the object referenced by the owning pointer.

A closer similarity is found in the pure functional language Clean,
which contains a sophisticated system of uniqueness
typing~\cite{barendsen1996uniqueness}.  Clean employs uniqueness
typing to re-use memory in cases where a function receives a unique
argument, but also (and perhaps more importantly) to control side
effects including arbitrary I/O.  As in \LO{}, alias analysis is used
to ensure that uniqueness properties are not violated.  A notable
difference is that the Clean language itself does not have any
facilities for consuming unique objects, apart from specifying a
function parameter as unique, but delegate this to (unsafe) internal
functions, that are exposed safely via the type system.  Furthermore,
a unique return value in Clean may alias some of the parameters to the
function, which is forbidden in \LO{}.  We have found that this
greatly simplifies analysis, and allows it to be fully
intraprocedural.

\section{Future Work}
\label{sec:future-work}

A very important immediate goal is the implementation of a code
generator targeting GPU execution.  Furthermore, a number of other
avenues for further research and development of \LO{} ae available.

\subsection{Size Information in Type System}

The size analysis presented in \cref{chap:hindrance-removal} is quite
restricted, and was designed and extended on an ad-hoc basis in order
to enable fusion of the real-world benchmarks.  Considering the great
importance of accurate size information in not only doing high-level
optimisations, but also generating efficient low-level code, it
appears very worthwhile to integrate tracking of array sizes into the
language itself.

We suggest a type system extension inspired by \textit{dependent
  types}, although much simpler.  As an example, let us look at how we
would like to be able to define matrix multiplication:

\begin{colorcode}
fun [[int,N],P] matMult([[int,N],M] a, [[int,M],P] b ) =
  ...
\end{colorcode}

This function declares that it takes two \texttt{int} array arguments,
the first of size $N \times M$ and the second of size $M \times P$,
and returns an integer array of size $N \times P$.  Any caller of
\texttt{matMult} must first prove to the type system that the
arguments have the correct size, while \texttt{matMult} itself must
prove that its body always returns an array of the appropriate size.

Such a proof could be provided through a mechanism much like the
current \texttt{assert}, which allows us a sort of ``escape hatch''
for when we cannot statically guarantee the size of our data - for
example, when it is given to us as input from the outside world, or
the result of a \texttt{filter}.  As the current \LO{} compiler is
already able to optimise and hoist many assertions away, this would be
useful by itself.

However, this would not solve the problem encountered in
\cref{chap:optimisation-results}, when the inability to transform a
\texttt{size} expression prevented fusion in program R0.  The
problematic part of R0 has this essential structure (where \texttt{N}
is some variable in scope):

\begin{colorcode}
let b = map(fn [real] (int x) =>
              let xa = replicate(N,x) in
              loop (xa) = for i < N do
                let xa = f(xa) // Does not change size of xa
                in
              xa,
            a) in
let n = size(1,b) in
map(g(n), b)
\end{colorcode}

The current ad-hoc size analyser is not smart enough to figure out the
inner size (\texttt{N}) of the array \texttt{b}.  Integrating size
information into the type system would allow us to annotate the return
type of the anonymous function as follows:

\begin{colorcode}
let b = map(fn [real,\emp{N}] (int x) =>
              ...,
            a) in
...
\end{colorcode}

We now statically promise that the inner size of \texttt{b} will
always be \texttt{N}, possibly backed by an assertion within the body
of the map.  The intent is that this promise can be checked by the
type-checker.  Presumably, for the body of the function to be
type-correct, the function \texttt{f} would have been defined to
return an array of the same size as its input.

It is not yet clear exactly how we should deal with cases where the
size of an array dimension cannot be statically known, or where it is
the result of a complex expression.  It is not desirable to support
the full power of dependent types, nor to include a full theorem
prover in \LO{}, as this could make it very cumbersome to use \LO{} as
a compiler target language.  In the end, it is important to remember
that our primary motivation is to improve size tracking for the
benefit of optimisation and code generation.

\subsection{Improved Aliasing Analysis}
\label{sec:improved-aliasing-analysis}

The system of uniqueness types presented in
\cref{chap:uniqueness-types} hinges crucially on tracking potential
sharing between arrays.  However, the current model of aliasing is
very coarse-grained, as it cannot describe sharing at a more precise
level than entire arrays.  For example, assume that we have the
following function:

\begin{colorcode}
  fun *[int] replace(*[int] arr, int i, int x) =
    let arr[i] = x in arr
\end{colorcode}

The result of \texttt{replace(a,i,x)} is the array \texttt{a} with the
element at index \texttt{i} replaced by \texttt{x}.  We may want to
use this function to replace an element within a slice of an array,
like so\footnote{Of course, in this contrived example, we could just
  use \texttt{let-with} rather than a separate function.}:

\begin{colorcode}
let b = a with [j] <- replace(a[j], j, x) in
...
\end{colorcode}

Unfortunately, this code will be refused by the compiler: The call to
\texttt{replace} consumes the array \texttt{a}, because \texttt{a[j]}
is aliased with \texttt{a}, yet \texttt{a} is used as the source in a
let-binding.  Making a separate binding for the call to
\texttt{replace} may make things more clear:

\begin{colorcode}
let r = replace(a[j], j, x) in \emp{// Consumes a}
let b = a with [j] <- r in
...
\end{colorcode}

As far as the type system is concerned, both the call to
\texttt{replace} and the \texttt{let-with} expression consume the
entirety of \texttt{a}, hence causing a compile-time
double-consumption error.  The only solution is to use \texttt{copy}:

\begin{colorcode}
let r = replace(\emphh{copy}(a[j]), j, x) in \emp{// Consumes a}
let b = a with [j] <- r in
...
\end{colorcode}

It is clear to us however, that the call to \texttt{replace} only
modifies the memory associated with the \texttt{j}th row of
\texttt{a}.  Furthermore, when \texttt{a} is next accessed (and
consumed), the \texttt{j}th row is replaced anyway.  Thus, it should
be possible to perform this entire operation in $O(1)$ space.

We could of course make a specialised variant of \texttt{replace} for
two-dimensional arrays, but this leads to unnecessary code bloat.
Thus, we believe that more precising tracking of sharing would be
worthwhile.  The problem is not easy, however, as the expression for
the array index may be arbitrarily complicated.

\subsection{Array Views}

Array indexing in \LO{} is quite limited - only entire dimensions can
be extracted.  With \texttt{split}, we can get slightly more control,
but extracting e.g. the inner elements of a $2$-dimensional array is
an enormously clumsy affair, as illustrated on
\cref{fig:inner-elements-l0}.

\begin{figure}
\centering
\begin{subfigure}[t]{.5\textwidth}
\begin{colorcode}
fun [[int]] inner([[int]] a) =
  let n = size(0,a) in
  let m = size(0,b) in
  let \{_,a2\}   = split(1,a) in
  let \{rows,_\} = split(n-2,a2) in
  map(fn [int] ([int] row) =>
        let \{_, row2\} =
          split(1,row) in
        let \{res, _\}  =
          split(m-2,row2) in
        res,
      rows)
\end{colorcode}
\subcaption{\LO{} \label{fig:inner-elements-l0}}
\end{subfigure}\hfill
\begin{subfigure}[t]{.4\textwidth}
\begin{colorcode}
def inner(a):
  a[1:-1, 1:-1]










\end{colorcode}
\subcaption{Numpy \label{fig:inner-elements-numpy}}
\end{subfigure}
\caption{Removing the outer elements of a $2$-dimensional array}
\label{fig:inner-elements}
\end{figure}

The primary reason for this is that \texttt{split} only slices the
outer dimension.  In other systems, such as the Python library
Numpy~\cite{oliphant2006guide}, it is comparatively much simpler to
simultaneously slice on every dimension of an array, as illustrated on
\cref{fig:inner-elements-numpy}.

As an array-oriented programming language, \LO{} should have similar
convenient support for slicing arrays.  It is not only practical when
writing code, but the resulting slice expressions are much easier to
analyse than a dense expression using \texttt{size} and
\texttt{split}.

More radically, many operations in \LO{} are operationally just
transformations of the index space of an underlying array.
Transpositions, \texttt{reshape}, \texttt{replicate}, and even array
indexing, merely provide different views of underlying data.  Thus,
perhaps the best solution would be to provide a language construct
that can express this mapping directly.  We can envision a syntax like
the following:

\begin{colorcode}
fun [[int]] transpose([[int]] a) =
  \emphh{arrange} a as
    size (n,m) => (m,n) \emp{// Determine size of output array}
    elem [i,j] => [j,i] \emp{// Map position (i,j) in output to position (j,i) in input}
\end{colorcode}

A crucial property is that every element of the output array maps
directly to some element in the input array, which means that at
compile time, we can remove the \texttt{arrange} intermediary and
access the original array directly.  This design is very similar to
the delayed arrays of Repa~\cite{keller2010regular}, which represent
arrays as a function from the index space to the value space.  The
built-in \texttt{replicate} could be reformulated as follows:

\begin{colorcode}
fun [[[int]]] replicate(int k, [[int]] r) =
  \emphh{arrange} r as
    size (n,m)   => (k,n,m)
    elem [i,j,p] => [j,p]
\end{colorcode}

We can also express entirely novel transformations, such as a
rearrangement that repeats every element of its input array twice:

\begin{colorcode}
fun [int] dupElem([int] a) =
  \emphh{arrange} a as
    size (n)     => (n)
    elem (2*i)   => [i]
    elem (2*i+1) => [i]
\end{colorcode}

A large potential problem with a construct such as \texttt{arrange} is
whether the compiler will be able to recognise ``known''
transformations.  For example, we demonstrated in
\cref{chap:fusion-enabling-soac-transformations} that the fusion
algorithm depends on being able to recognise and rewrite
\texttt{transpose} expressions, which it must be able to do, even if
they are formulated in terms of \texttt{arrange}.  Hence, it would be
important that every \texttt{arrange} can be reduced to a canonical
form that uniquely represents the transformation it performs.

\subsection{Software Engineering}

The world already has plenty of papers and theses stuffed with long
listings of Haskell code, and we have therefore tried to shy away from
talking too much about the software architecture of the \LO{}
compiler.  While the overall code base is healthy and well-structured,
there are still several instances of technical debt that should be
paid off:

\begin{itemize}
\item While the compiler is nicely divided into discrete passes, the
  order in which said passes should be invoked is a bit unclear.  As
  it stands, programs are passed through every pass several times,
  simply to ensure that they get optimised fully.  This requires some
  bit of re-engineering, probably also involving changing some passes
  (particularly the fusion module) to be less sensitive as to the
  shape of the input program.

\item For this thesis, \LO{} has been divided into an external and
  internal language.  In the compiler, both of these are included in
  the same abstract syntax tree definition, with most passes either
  silently ignoring or loudly crashing if they encounter a construct
  that belongs to the external language.  This creates undue
  complexity, and should be resolved by splitting the language more
  clearly, even if the cost is some code duplication (for example, we
  might need separate but very similar parsers).

\item Somewhat related to the previous issue, the \LO{} syntax tree
  definition does not statically enforce normalisation.  Again,
  compiler passes either ignore them or crash when an un-normalised
  term is encountered.

\item Many optimisations depend on every variable in the program
  posessing a unique name.  This property is ensured by tagging each
  input name with a unique integer, then passing around a counter that
  can be used to generate fresh, globally unique integers.
  Unfortunately, many transformations (e.g. inlining) end up
  duplicating bits of code, which then have to be entirely renamed in
  order to preserve uniqueness.  Furthermore, passing the counter
  around is cumbersome, even if packaged in a state monad.  An
  alternative approach to handling name binding, based on de Bruijn
  indices~\cite{McBride:2004:FPI:1017472.1017477}, is being
  considered.  Such an approach would allow us to get rid of the
  counter, while still being able to cheaply avoid unwanted name
  capture.
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

In this masterâ€™s thesis, we have presented the design of a pure
functional data-parallel language, with a design that enables both (i)
a degree low-level imperative programming, as well as (ii) supporting
high-level structural transformations such as loop fusion.  The
language contains a type system for in-place modification and aliasing
of arrays and array slices that ensures referential transparency,
which in turn supports equational reasoning.

Previous work on fusion has taken two main directions: Either fusion
is performed aggressively, and the programmer is provided primitives
to inhibit fusion, for example by forcing array to materialise, or
fusion is performed via rewriting rules on the syntax tree.  The
latter approach relies tightly on the inliner engine, and its
applicability is limited to the case when each fused array is consumed
by one array combinator.

This thesis has presented a program-level, structural-analysis
approach to fusion that handles the difficult case in which an array
produced by a second-order array combinator (SOAC), such as
\texttt{map}, is consumed by several other SOACs (if the SOAC
producer-consumer dependency graph is reducible).  This essentially
allows fusion to operates across \texttt{zip}/\texttt{unzip}.

Furthermore, we have shown a compositional algebra for fusion that
includes array combinators, such as \texttt{map}, \texttt{reduce},
\texttt{filter}, \texttt{scan}, and \texttt{redomap}, and other
built-in functions that would otherwise hinder fusion applicability,
such as \texttt{size}, \texttt{transpose}, and \texttt{reshape}.  This
algebra also includes transformations that in come cases allow fusion
with \texttt{scan} as both consumer and producer.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis.tex"
%%% End: 
